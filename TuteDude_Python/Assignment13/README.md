# Assignment 13: Web Scraping Project  

## 📌 Module  
**Module 17 – Web Scraping Implementation**  

---

## 📄 Description  
This assignment demonstrates **web scraping with Python** using `requests`, `BeautifulSoup`, and file handling.  
It includes scripts for extracting data from web pages, filtering useful information, working with GET & POST requests, downloading images, and saving structured results into CSV files.  

---

## 📂 Project Structure  

```plaintext
webscrapping/
│── content.py
│── downloading_images.py
│── filtering_the_data.py
│── getting_data.py
│── params.py
│── post_the_data.py
│── price_tracing.py
│── text.py
│── to_csv_file.py
│── web.py
│── wikipedia_headings.csv   # Output file generated by to_csv_file.py
│── python.png
│
├── moon/        # Downloaded images related to "moon"
│   ├── 250px-Supermoon_Nov-14-2016-minneapolis.jpg
│   ├── 330px-FullMoon2010.jpg
│   ├── DMLwZCcWy25RhSYoszqsjN.jpg
│   ├── GettyImages-2233752217-1024x683.jpg
│   └── hero-image.fill.size_1248x702.v1756735140.jpg
│
└── nature/      # Downloaded images related to "nature"
    ├── 0ZUBmNNVLRCfn3NdU55nQ00UF64m2ObtcDS0grx02fA.jpg
    └── 1200px-Altja_j%C3%B5gi_Lahemaal.jpg
```
## 📝 Explanation of Files  

- **`content.py`**  
  This script focuses on fetching a webpage and displaying its raw HTML content.  
  It demonstrates how to use the `requests` library to send an HTTP request and retrieve response data.  
  The file is useful for understanding the structure of the page before applying filters or parsing.  
  It helps build the foundation for advanced scraping by showing the unprocessed data.  

- **`downloading_images.py`**  
  This script extracts image URLs from web pages and downloads them to local folders.  
  It saves images into two output folders – `moon/` and `nature/` – based on the category.  
  The script uses `requests` for fetching images and file handling for saving them.  
  This is an example of **media scraping**, often used in automation tasks like dataset creation.  

- **`filtering_the_data.py`**  
  This script demonstrates how to clean and filter unwanted tags or elements from HTML content.  
  Using BeautifulSoup, it selects specific tags such as paragraphs, links, or headings.  
  It shows the power of HTML parsing to extract only meaningful data from cluttered pages.  
  This approach is helpful for focused scraping, like extracting only news headlines.  

- **`getting_data.py`**  
  This script shows how to use HTTP GET requests for retrieving webpage content.  
  It sends a request to a target URL and displays the server’s response.  
  The code introduces basic error handling to manage failed requests or invalid URLs.  
  It forms the core of most scraping tasks, as GET is the most commonly used request method.  

- **`params.py`**  
  Demonstrates how to pass query parameters with a GET request.  
  This simulates search or API calls, where additional parameters filter or customize results.  
  The script prints the final request URL so users can see how queries are appended.  
  It’s particularly useful for scraping sites with search functionality or APIs.  

- **`post_the_data.py`**  
  This script illustrates how to send data to a server using POST requests.  
  It includes building a payload (dictionary) and sending it securely to a target URL.  
  The script shows how web forms work in the backend, making it important for automation.  
  POST requests are commonly used when submitting forms or uploading information online.  

- **`price_tracing.py`**  
  This script is built for scraping product price details from e-commerce sites.  
  It fetches the relevant HTML elements containing price information and extracts values.  
  The goal is to track prices dynamically for analysis or comparison.  
  Such a scraper is useful for projects like monitoring discounts or stock changes.  

- **`text.py`**  
  This script extracts clean textual information from a webpage.  
  It removes tags and formatting to display only the human-readable text.  
  The output can then be processed for natural language analysis or stored for research.  
  It demonstrates how to filter meaningful content out of raw HTML responses.  

- **`to_csv_file.py`**  
  This script scrapes headings from a Wikipedia page and writes them into a CSV file.  
  It processes the HTML, extracts required tags, and saves them using Python’s `csv` module.  
  The output is stored in **`wikipedia_headings.csv`** for structured data access.  
  This showcases converting unstructured web data into a structured format for reuse.  

- **`web.py`**  
  A general-purpose scraper to demonstrate parsing with BeautifulSoup.  
  It loads a webpage, locates elements by tags or attributes, and displays their content.  
  This file acts as a sandbox for experimenting with different scraping techniques.  
  It is a good starting point for testing before implementing advanced scripts.  

- **`wikipedia_headings.csv`**  
  The CSV file is the output of `to_csv_file.py`.  
  It contains structured headings scraped directly from Wikipedia.  
  This file demonstrates how scraped data can be stored for later analysis.  
  The approach makes the scraped results portable and easy to process.  
  
---

## 📑 Sample Preview of `wikipedia_headings.csv`  

| Heading Level | Heading Text          |
|---------------|----------------------|
| h1            | Web scraping         |
| h2            | History              |
| h2            | Techniques           |
| h2            | Legal issues         |
| h3            | United States        |

*(Output may vary depending on Wikipedia updates.)*  

---

## 📷 Output Folders  

### 🌓 Moon Images  
The `moon/` folder contains scraped images of the moon.  

### 🌿 Nature Images  
The `nature/` folder contains scraped images of natural sceneries.  

### 📑 CSV Output  

The `wikipedia_headings.csv` file contains extracted headings from a Wikipedia page.  



